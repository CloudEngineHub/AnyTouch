<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="AnyTouch: Learning Unified Static-Dynamic Representation across Multiple Visuo-tactile Sensors">
    <meta name="keywords" content="Multi-Sensory, Robotic Manipulation, Multi-Stage">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AnyTouch: Learning Unified Static-Dynamic Representation across Multiple Visuo-tactile Sensors</title>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true } });
    </script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <!-- <link rel="icon" href="./static/images/logo.png"> -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        .card {
            box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2);
            /* transition: 0.3s; */
            /* width: 300px; */
            border-radius: 10px;
            /* 圆角的大小 */
            /* margin: 50px; */
            padding: 7px;
            background-color: #f1f1f1;
        }

        .card:hover {
            box-shadow: 0 8px 16px 0 rgba(0, 0, 0, 0.2);
        }

        .card2 {
            box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2);
            /* transition: 0.3s; */
            /* width: 300px; */
            border-radius: 7px;
            /* 圆角的大小 */
            /* margin: 50px; */
            padding: 5px;
            background-color: #f1f1f1;
        }

        .card2:hover {
            box-shadow: 0 8px 16px 0 rgba(0, 0, 0, 0.2);
        }
    </style>
</head>

<body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu">
            <div class="navbar-start" style="flex-grow: 1; justify-content: center;">

                <div class="navbar-item has-dropdown is-hoverable">
                    <a class="navbar-link">
                        More Research
                    </a>
                    <div class="navbar-dropdown">
                        <a class="navbar-item" href="https://gewu-lab.github.io/">
                            GeWu Lab@RUC
                        </a>

                    </div>
                </div>
            </div>

        </div>
    </nav>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">AnyTouch: Learning Unified Static-Dynamic
                            Representation across Multiple Visuo-tactile Sensors</h1>
                        <h3 class="title is-5 publication-title">International Conference on Learning Representations
                            (ICLR) 2025</h3>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://xxuan01.github.io/">Ruoxuan
                                    Feng</a><sup>1</sup>,</span>
                            <span class="author-block">
                                Jiangyu Hu<sup>2,3</sup>,</span>
                            <span class="author-block">
                                <a href="https://xwinks.github.io/">Wenke Xia</a><sup>1</sup>,</span>
                            <span class="author-block">
                                Tianci Gao<sup>1</sup>,</span>
                            <span class="author-block">
                                Ao Shen<sup>1</sup>,</span>
                            <span class="author-block">
                                <a href="https://scholar.google.com.hk/citations?user=ShKpk00AAAAJ">Yuhao
                                    Sun</a><sup>3</sup>,</span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=5G47IcIAAAAJ">Bin
                                    Fang</a><sup>3</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://dtaoo.github.io/">Di Hu</a><sup>1</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Renmin
                                University of China &nbsp; &nbsp; <sup>2</sup>Wuhan University of Science and
                                Technology</span><br />
                            <span class="author-block"><sup>3</sup>
                                Beijing University of Posts and Telecommunications</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://openreview.net/pdf?id=XToAemis1h"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <!-- <span class="link-block">
                                    <a href="https://rick-xu315.github.io/ICASSP23_Sup.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Supplementary</span>
                                    </a>
                                </span> -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2408.01366v2"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/GeWu-Lab/AnyTouch"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                                <!-- Dataset Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/google/nerfies/releases/tag/0.1"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fa fa-database main-icon"></i>
                                        </span>
                                        <span>Dataset</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://github.com/GeWu-Lab/AnyTouch"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fa fa-dice-d6"></i>
                                        </span>
                                        <span>Checkpoint</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
    </section>


    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3 has-text-centered">Overview</h2>
                <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
              <source src="./static/images/method.pdf">
            </video> -->
                <!-- <embed src="./static/images/method.pdf" type="application/pdf"> -->
                <!-- <embed src="./static/images/all.pdf" type="application/pdf" width="100" height="100"> -->
                <img src='./static/images/intro.png' width="100%" height="100%">
                <br><br>
                <div class="content has-text-justified">
                    <p>
                        Tactile perception is crucial for humans to perceive the physical world. Over the years, various
                        visuo-tactile sensors have been designed to endow robots with human-like tactile perception
                        abilities. However, the low standardization of visuo-tactile sensors has hindered the
                        development of a powerful tactile perception system. In this work, we present <b>TacQuad</b>, an
                        aligned multi-modal multi-sensor tactile dataset that enables the explicit integration of
                        sensors. Building on this foundation and other open-sourced tactile datasets, we propose
                        learning unified representations from both static and dynamic perspectives to accommodate a
                        range of tasks. We introduce <b>AnyTouch</b>, a unified static-dynamic multi-sensor tactile
                        representation learning framework with a multi-level architecture, enabling comprehensive static
                        and real-world dynamic tactile perception.
                    </p>
                </div>
            </div>
        </div>
    </section>


    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3 has-text-centered">Method</h2>
                <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
              <source src="./static/images/method.pdf">
            </video> -->
                <!-- <embed src="./static/images/method.pdf" type="application/pdf"> -->
                <img src='./static/images/model.png' width="100%" height="100%">
                <br><br>
                <div class="content has-text-justified">
                    <p> To deal with the above challenge, we propose <b>MS-Bot</b>, a stage-guided dynamic multi-sensory
                        fusion method with coarse-to-fine stage understanding.
                        We first add a stage label \( s_t \) for each sample to form \( (\textbf{X}_t, a_t, s_t) \),
                        where \( \textbf{X}_t \) is the multi-sensory observation at timestep \( t \) and \( a_t \) is
                        the action label. We then train the MS-Bot, which consists of four components:
                    </p>
                    <ul>
                        <li><b>Feature Extractor:</b> This component consists of several uni-modal encoders. Each
                            encoder takes a brief history of observations \( X_t^m \in \mathbb{R}^{T\times H_m \times
                            W_m
                            \times C_m} \) of the modality \( m \) as input, where \( T \) is the timestep number of the
                            brief
                            history and \( H_m, W_m, C_m \) indicates the input shape of modality \( m \). These
                            observations
                            are then encoded into feature tokens \( \mathbf{f}_t \in \mathbb{R}^{M\times T \times d} \)
                            where \( d \) denotes dimension.</li>
                        <li><b>State Tokenizer:</b> This component aims to encode the observations and action history \(
                            (a_1, a_2,...,a_{t-1}) \) into a token that can represent the current state. Action history
                            is similar to human memory and can help to indicate the current state within the whole task.
                            We input the action history as a one-hot sequence into an LSTM,
                            then concatenate the output with the feature tokens and encode them into a state token
                            \( z^{state}_t \) through a Multi-Layer Perceptron (MLP). </li>
                        <li><b>Stage Comprehension Module:</b> This module aims to perform coarse-to-grained stage
                            understanding by injecting stage information into the state token. For a task with $S$
                            stages, we use \( S \) learnable stage tokens \( [stage_1],...,[stage_S] \) to represent
                            each stage.
                            We then use a gate network (MLP) to predict the current stage, then multiply the softmax
                            score \( \mathbf{g}_t \) with the stage tokens and sum them up to obtain the stage token \(
                            z^{stage}_t \) at
                            timestep \( t \):
                            $$
                            \begin{equation}
                            \begin{gathered}
                            \mathbf{g}_t = (g^{1}_t,...,g^{S}_t) = softmax (MLP (z^{state}_t)),\\
                            z^{stage}_t = \frac{1}{S} \sum_{j=1}^S(g^{j}_t \cdot [stage_j]).
                            \end{gathered}
                            \end{equation}
                            $$
                            Finally, we compute the weighted sum of the state token $z^{state}_t$ and the current stage
                            token $z^{stage}_t$ using a weight $\beta$ to obtain the stage-injected state token
                            $z^{*}_t$:
                            \begin{equation}
                            z^{*}_t = \beta \cdot z^{state}_t + (1-\beta) \cdot z_t^{stage}.
                            \end{equation}
                            Different from the old state token $z^{state}_t$, the new state token $z^{*}_t$ represents
                            the fine-grained state within a stage. In this case, $z^{stage}_t$ is regarded as an anchor
                            stage, while $z^{state}_t$ can indicate the shift inside the stage, thereby achieving
                            coarse-to-fine stage comprehension.
                            During the training process, we utilize stage labels to supervise the stage scores output by
                            the gate net.
                            We use a soft penalty loss $\mathcal{L}_{gate}$ to constrain the output of the gate net on
                            the $i$-th sample:
                            \begin{equation}
                            \begin{gathered}
                            \mathcal{L}_{gate,i} = \sum_{j=1}^S (w_i^j\cdot g_i^j), \ j \in \{ 1,2,...,S \}, \\
                            w_i^j = \left \{
                            \begin{array}{ll}
                            0, & (s_i = j) \ or \ (\exists k,\ |k-i| \leq \gamma,\ s_i \ne s_k)
                            , \\
                            1, & otherwise,
                            \end{array}
                            \right. \\
                            \end{gathered}
                            \end{equation}
                            where $k$ indicates a nearby sample in the same trajectory, $s_i$ and $s_k$ represent stage
                            labels and $\gamma$ is a hyper-parameter used to determine the range near the stage
                            boundaries.
                        </li>
                        <li>
                            <b>Dynamic Fusion Module:</b> We aim for this module to dynamically select the modalities of
                            interest based on the fine-grained state within the current stage. We use the state token
                            with stage information $z^{*}_t$ as query, and the feature tokens $\mathbf{f}_t$ as key and
                            value for cross-attention. The features from all modalities are integrated into a fusion
                            token $z^{fus}_t$ based on the current stage's requirements. Finally, the fusion token
                            $z^{fus}_t$ is fed into an MLP to predict the next action $a_t$. We also introduce random
                            attention blur by replacing the attention scores on feature tokens with the same average
                            value $\frac{1}{M\times T}$ with a probability $p$ to prevent the model from simply
                            memorizing the actions corresponding to attention score patterns.
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3 has-text-centered">Experiments</h2>
                <div class="content has-text-justified">
                    <p>We evaluate our method on two challenging robotic manipulation tasks: pouring and peg insertion
                        with keyway.
                        We compare our method with three baselines in both tasks:
                    </p>
                    <ol>
                        <li><b>Concat:</b> a model which directly
                            concatenates all the uni-modal
                            features. </li>
                        <li><b>Du et al.:</b> a model that uses LSTM to fuse the uni-modal features and the additional
                            proprioceptive information. </li>
                        <li><b>MULSA:</b> a model which fuses the uni-modal features via self-attention. </li>
                    </ol>
                    <p>We also compare our method with two variants in each task:
                    </p>
                    <ol>
                        <li><b>MS-Bot (w/o A/D):</b> removing audio in pouring and depth in peg insertion for MS-Bot.
                        </li>
                        <li><b>MS-Bot (w/o T/R):</b> removing touch in pouring and RGB in peg insertion for MS-Bot.
                        </li>
                    </ol>

                </div>
                <!-- <p>The results are shown in Table 1:</p> -->
                <!-- <br></br> -->
                <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
              <source src="./static/images/method.pdf">
            </video> -->
                <!-- <embed src="./static/images/method.pdf" type="application/pdf"> -->
                <img src='./static/images/result1.png' style="margin-top: 15px;">
                <br><br>
                <!-- <h2 class="subtitle has-text-justified">
                    $\dagger$ indicates MMCosine is applied. Combined with MMCosine, most of the fusion methods gain
                    considerable improvement for datasets of various scales, domains, and label amount.
                </h2> -->
                <h2 class="title is-4">Visualization</h2>
                <img src='./static/images/result2.png'>
                Visualization of the aggregated attention scores for each modality and stage scores in the pouring task.
                At each timestep, we average the attention scores on all feature tokens of each modality separately. The
                stage score is the output of the gate network after softmax normalization.




            </div>
        </div>
    </section>

    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{feng2024play,
    title={Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic Manipulation},
    author={Feng, Ruoxuan and Hu, Di and Ma, Wenke and Li, Xuelong},
    booktitle={8th Annual Conference on Robot Learning},
    year={2024},
    url={https://openreview.net/forum?id=N5IS6DzBmL}
}</code></pre>
    </div>


    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="https://openreview.net/pdf?id=XToAemis1h">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://github.com/GeWu-Lab/AnyTouch" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">

                        <p>
                            Thanks to <a href="https://nerfies.github.io/">Nerfies</a> for providing the
                            template of this page.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>
</body>

</html>